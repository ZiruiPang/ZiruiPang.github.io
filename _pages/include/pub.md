# üìù Publications 
## ü§ñ LLM Unlearning
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">under review</div><img src='images/sga_pipeline.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Label Smoothing Improves Gradient Ascent in LLM Unlearning](https://arxiv.org/pdf/2510.22376)

**Zirui Pang**, Hao Zheng, Zhijie Deng, Ling Li, Zixin Zhong, Jiaheng Wei

- We identify the instability of Gradient Ascent in LLM unlearning.
- We propose Smoothed Gradient Ascent (SGA) with a tunable smoothing rate.
- SGA achieves more stable and effective unlearning across benchmarks.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">under review</div><img src='images/offside_pipeline_v6.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[OFFSIDE: Benchmarking Unlearning Misinformation in Multimodal Large Language Models](https://arxiv.org/abs/2510.22535)

Hao Zheng, **Zirui Pang**, Ling li, Zhijie Deng, Yuhan Pu, Zhaowei Zhu, Xiaobo Xia, Jiaheng Wei

[**Project**](https://github.com/zh121800/OFFSIDE) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>

- We present OFFSIDE, a benchmark for multimodal unlearning based on football transfer rumors.
- It provides real-world, manually curated data and four evaluation settings to test forgetting, utility, and robustness.
- Our results reveal that current methods fail to unlearn visual rumors and are vulnerable to recovery and prompt attacks.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">under review</div><img src='images/guard_pipeline.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection](https://arxiv.org/abs/2505.13312)

Zhijie Deng, Chris Yuhao Liu, **Zirui Pang**, Xinlei He, Lei Feng, Qi Xuan, Zhaowei Zhu, Jiaheng Wei

- We propose GUARD, a generation-time unlearning framework for LLMs.
- It detects forget-related prompts and blocks forbidden tokens during generation.
- GUARD achieves effective forgetting without harming model utility.
</div>
</div>


## üèûÔ∏è Label Noise

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">under review</div><img src='images/reveal_pipeline.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[When VLMs Meet Image Classification: Test Sets Renovation via Missing Label Identification](https://arxiv.org/pdf/2505.16149)

**Zirui Pang**, Haosheng Tan, Yuhan Pu, Zhijie Deng, Zhouan Shen, Keyu Hu, Jiaheng Wei

[**Project**](https://github.com/ZiruiPang/REVEAL-label-renovation) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>

- We propose REVEAL, a framework that uses vision-language models to find and fix missing and noisy labels in image classification benchmarks.
- It ensembles multiple VLMs and human feedback to renovate test sets with soft, accurate labels.
- REVEAL greatly improves dataset quality and aligns closely with human judgments.
</div>
</div>


